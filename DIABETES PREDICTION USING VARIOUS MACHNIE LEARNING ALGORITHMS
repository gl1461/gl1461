#!/usr/bin/env python
# coding: utf-8

# In[186]:


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')


# In[187]:


df=pd.read_csv("diabetes.csv")


# In[188]:


df.head()


# In[189]:


df.tail()


# In[190]:


df.sample(10)


# In[191]:


df.shape


# In[192]:


df.dtypes


# In[193]:


df.info()


# In[194]:


df.describe()


# In[195]:


#before drop the duplicates
df.shape


# In[196]:


df=df.drop_duplicates()


# In[197]:


#after drop the duplicates
df.shape


# In[198]:


#checking of null values
df.isnull().sum()


# In[199]:


df.columns


# In[200]:


#checking no of zero values in the dataset
print('No. of Zero values in Glucose',df[df['Glucose']==0].shape[0])


# In[201]:


print('No. of Zero values in BloodPressure',df[df['BloodPressure']==0].shape[0])


# In[202]:


print('No. of Zero values in SkinThickness',df[df['SkinThickness']==0].shape[0])


# In[203]:


print('No. of Zero values in Insulin',df[df['Insulin']==0].shape[0])


# In[204]:


print('No. of Zero values in BMI',df[df['BMI']==0].shape[0])


# In[205]:


#replacing zeros with mean of that columns
df['Glucose']=df['Glucose'].replace(0,df['Glucose'].mean())
print('No. of Zero values in Glucose',df[df['Glucose']==0].shape[0])


# In[206]:


df['BloodPressure']=df['BloodPressure'].replace(0,df['BloodPressure'].mean())
df['SkinThickness']=df['SkinThickness'].replace(0,df['SkinThickness'].mean())
df['Insulin']=df['Insulin'].replace(0,df['Insulin'].mean())
df['BMI']=df['BMI'].replace(0,df['BMI'].mean())


# In[207]:


df.describe()


# In[208]:


#Histogram of each feature
df.hist(bins=10,figsize=(10,10))
plt.show()


# In[209]:


#Scatter plot matrix
from pandas.plotting import scatter_matrix
scatter_matrix(df, figsize=(20,20));


# In[210]:


#pairplot
sns.pairplot(data=df,hue='Outcome')
plt.show()


# In[211]:


#Analyzing relationships between variables
#correlation analysis
import seaborn as sns
corrmat=df.corr()
top_corr_features=corrmat.index
plt.figure(figsize=(10,10))
g=sns.heatmap(df[top_corr_features].corr(),annot=True,cmap="RdYlGn")


# In[212]:


#splitting the data frame into X and Y
target_name='Outcome'
y=df[target_name]
x=df.drop(target_name,axis=1)


# In[213]:


x.head()


# In[214]:


y.head()


# In[215]:


#Applying Feature Scalling
#feature scalling techniques are of 4 types
#Standard Scaler
#Normalizer
#minmax scaler
#binarizer
#applying standard scaler
from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()
scaler.fit(x)
SSX=scaler.transform(x)


# In[216]:


#dividing data into trainging data which is 80% and testing data which is 20%
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(SSX,y,test_size=0.2,random_state=7)


# In[217]:


x_train.shape,y_train.shape


# In[218]:


x_test.shape,y_test.shape


# In[219]:


#Buliding the algorithms
#LOGISTIC REGRESSION
from sklearn.linear_model import LogisticRegression
lr=LogisticRegression(solver='liblinear',multi_class='ovr')
lr.fit(x_train,y_train)


# In[220]:


#KNeighborsClassifer(KNN)
from sklearn.neighbors import KNeighborsClassifier
knn=KNeighborsClassifier()
knn.fit(x_train,y_train)


# In[221]:


#Naive-Bayes Classifier
from sklearn.naive_bayes import GaussianNB
nb=GaussianNB()
nb.fit(x_train,y_train)


# In[222]:


#Support Vector Machine(SVM)
from sklearn.svm import SVC
sv=SVC()
sv.fit(x_train,y_train)


# In[223]:


#Decision tree
from sklearn.tree import DecisionTreeClassifier
dt=DecisionTreeClassifier()
dt.fit(x_train,y_train)


# In[224]:


#Random Forest
from sklearn.ensemble import RandomForestClassifier
rf=RandomForestClassifier(criterion='entropy')
rf.fit(x_train,y_train)


# In[225]:


#Making Prediction
#making predictions by using logistic regression
lr_pred=lr.predict(x_test)


# In[226]:


#making predictions by using KNN
knn_pred=knn.predict(x_test)


# In[227]:


#making predictions by using Naive Bayes
nb_pred=nb.predict(x_test)


# In[228]:


#making predictions by using SVM
sv_pred=sv.predict(x_test)


# In[229]:


#making predictions by using Decision tree
dt_pred=dt.predict(x_test)


# In[230]:


#making predictions by using Random Forest
rf_pred=rf.predict(x_test)


# In[231]:


#Model Evaluation
#train score and test score of Logistic Regression
from sklearn.metrics import accuracy_score
print("Train Accuracy of Logistic Regression",lr.score(x_train,y_train)*100)
print("Accuracy test score of Logistic Regression",lr.score(x_test,y_test)*100)
print("Accuracy score of Logistic Regression",accuracy_score(y_test,lr_pred)*100)


# In[232]:


#train score and test score of KNN
print("Train Accuracy of KNN",knn.score(x_train,y_train)*100)
print("Accuracy test score of KNN",knn.score(x_test,y_test)*100)
print("Accuracy score of KNN",accuracy_score(y_test,knn_pred)*100)


# In[233]:


#train score and test score of Naive-Bayes
print("Train Accuracy of Naive Bayes",nb.score(x_train,y_train)*100)
print("Accuracy test score of Naive Bayes",nb.score(x_test,y_test)*100)
print("Accuracy score of Naive Bayes",accuracy_score(y_test,nb_pred)*100)


# In[234]:


#train score and test score of SVM
print("Train Accuracy of SVM",sv.score(x_train,y_train)*100)
print("Accuracy test score of SVM",sv.score(x_test,y_test)*100)
print("Accuracy score of SVM",accuracy_score(y_test,sv_pred)*100)


# In[235]:


#train score and test score of Decision Tree
print("Train Accuracy of Decision Tree",dt.score(x_train,y_train)*100)
print("Accuracy test score of Decision Tree",dt.score(x_test,y_test)*100)
print("Accuracy score of Decision Tree",accuracy_score(y_test,dt_pred)*100)


# In[236]:


#train score and test score of RandomForest
print("Train Accuracy of Random Forest",rf.score(x_train,y_train)*100)
print("Accuracy test score of Random Forest",rf.score(x_test,y_test)*100)
print("Accuracy score of Random Forest",accuracy_score(y_test,rf_pred)*100)


# In[237]:


#Confusion matrix of logistic regression
from sklearn.metrics import classification_report,confusion_matrix
cm=confusion_matrix(y_test,lr_pred)


# In[238]:


sns.heatmap(confusion_matrix(y_test,lr_pred),annot=True,fmt="d")


# In[239]:


print('Classification Report of Logistic Regression: \n',classification_report(y_test,lr_pred,digits=4))


# In[240]:


TN=cm[0,0]
FP=cm[0,1]
FN=cm[1,0]
TP=cm[1,1]


# In[241]:


TN,FP,FN,TP


# In[242]:


#MAKING CONFUSION MATRIX OF LOGISTIC REGRESSION
from sklearn.metrics import classification_report,confusion_matrix
from sklearn.metrics import accuracy_score,roc_auc_score,roc_curve
cm=confusion_matrix(y_test,lr_pred)

print('TN - True Negative {}'.format(cm[0,0]))
print('FP - False Positive {}'.format(cm[0,1]))
print('FN - False Negative {}'.format(cm[1,0]))
print('TP - True Positive {}'.format(cm[1,1]))
print('Accuracy Rate: {}' .format(np.divide(np.sum([cm[0,0],cm[1,1]]),np.sum(cm))*100))
print('Misclassification Rate: {}' .format(np.divide(np.sum([cm[0,1],cm[1,0]]),np.sum(cm))*100))


# In[243]:


import matplotlib.pyplot as plt
plt.clf()
plt.imshow(cm,interpolation='nearest',cmap=plt.cm.Wistia)
classNames=['0','1']
plt.title('Confusion matrix of logistic regression')
plt.ylabel('Actual(true) values')
plt.xlabel('Predicted values')
tick_marks=np.arange(len(classNames))
plt.xticks(tick_marks,classNames,rotation=45)
plt.yticks(tick_marks,classNames)
s=[['TN','FP'],['FN','TP']]
for i in range(2):
    for j in range(2):
        plt.text(j,i,str(s[i][j])+"="+str(cm[i][j]))
plt.show()


# In[244]:


pd.crosstab(y_test,lr_pred,margins=False)


# In[245]:


pd.crosstab(y_test,lr_pred,margins=True)


# In[246]:


pd.crosstab(y_test,lr_pred,rownames=['Actual values'], colnames=['Predicted values'],margins=True)


# In[247]:


#Precision
TP,FP


# In[248]:


Precision=TP/(TP+FP)
Precision


# In[249]:


#print precision score
precision_score=TP/float(TP+FP)*100
print('Precision score: {0:0.4f}' .format(precision_score))


# In[250]:


from sklearn.metrics import precision_score
print("precision score is:", precision_score(y_test,lr_pred)*100)


# In[251]:


#F1 score
from sklearn.metrics import f1_score
print('f1_score:',f1_score(y_test,lr_pred)*100)


# In[252]:


#false positive rate(fpr)
FPR=FP/float(FP+TN)*100
print('False Positive Rate : {0:0.4f}' .format(FPR))


# In[253]:


#Specificity
specificity=TN/(TN+FP)*100
print('Specificity : {0:0.4f}' .format(specificity))


# In[254]:


#ROC Curve & ROC AUC
#Area under curve
auc= roc_auc_score(y_test,lr_pred)
print("ROC AUC_SCORE of Logistic Regression is",auc)


# In[255]:


fpr,tpr,thresholds=roc_curve(y_test,lr_pred)
plt.plot(fpr,tpr,color='orange',label='ROC')
plt.plot([0,1],[0,1],color='darkblue',linestyle='--',label='ROC curve(area=%0.2f)'%auc)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve of Logistic Regression')
plt.legend()
plt.grid()
plt.show()


# In[256]:


#confusion matrix of KNN
#MAKING CONFUSION MATRIX OF KNN
from sklearn.metrics import classification_report,confusion_matrix
from sklearn.metrics import accuracy_score,roc_auc_score,roc_curve
cm=confusion_matrix(y_test,knn_pred)

print('TN - True Negative {}'.format(cm[0,0]))
print('FP - False Positive {}'.format(cm[0,1]))
print('FN - False Negative {}'.format(cm[1,0]))
print('TP - True Positive {}'.format(cm[1,1]))
print('Accuracy Rate: {}' .format(np.divide(np.sum([cm[0,0],cm[1,1]]),np.sum(cm))*100))
print('Misclassification Rate: {}' .format(np.divide(np.sum([cm[0,1],cm[1,0]]),np.sum(cm))*100))


# In[257]:


#classification report of KNN
print('Classification Report of KNN: \n', classification_report(y_test,knn_pred,digits=5))


# In[258]:


#Area under curve of KNN
auc=roc_auc_score(y_test,knn_pred)
print("ROC AUC SCORE of KNN is",auc)


# In[259]:


fpr,tpr,thresholds=roc_curve(y_test,knn_pred)
plt.plot(fpr,tpr,color='orange',label='ROC')
plt.plot([0,1],[0,1],color='darkblue',linestyle='--',label='ROC curve(area=%0.2f)'%auc)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) KNN')
plt.legend()
plt.grid()
plt.show()


# In[260]:


#confusion matrix of Naive Bayes
#MAKING CONFUSION MATRIX OF Naive Bayes
from sklearn.metrics import classification_report,confusion_matrix
from sklearn.metrics import accuracy_score,roc_auc_score,roc_curve
cm=confusion_matrix(y_test,nb_pred)

print('TN - True Negative {}'.format(cm[0,0]))
print('FP - False Positive {}'.format(cm[0,1]))
print('FN - False Negative {}'.format(cm[1,0]))
print('TP - True Positive {}'.format(cm[1,1]))
print('Accuracy Rate: {}' .format(np.divide(np.sum([cm[0,0],cm[1,1]]),np.sum(cm))*100))
print('Misclassification Rate: {}' .format(np.divide(np.sum([cm[0,1],cm[1,0]]),np.sum(cm))*100))


# In[261]:


sns.heatmap(confusion_matrix(y_test,nb_pred),annot=True,fmt="d")


# In[262]:


#classification report of Naive Bayes
print('Classification Report of Naive Bayes: \n', classification_report(y_test,nb_pred,digits=5))


# In[263]:


#Area under curve of Naive Bayes
auc=roc_auc_score(y_test,nb_pred)
print("ROC AUC SCORE of Naive Bayes is",auc)


# In[264]:


fpr,tpr,thresholds=roc_curve(y_test,nb_pred)
plt.plot(fpr,tpr,color='orange',label='ROC')
plt.plot([0,1],[0,1],color='darkblue',linestyle='--',label='ROC curve(area=%0.2f)'%auc)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Naive Bayes')
plt.legend()
plt.grid()
plt.show()


# In[265]:


#confusion matrix of Support Vector Machine
#MAKING CONFUSION MATRIX OF Support Vector Machine
from sklearn.metrics import classification_report,confusion_matrix
from sklearn.metrics import accuracy_score,roc_auc_score,roc_curve
cm=confusion_matrix(y_test,sv_pred)

print('TN - True Negative {}'.format(cm[0,0]))
print('FP - False Positive {}'.format(cm[0,1]))
print('FN - False Negative {}'.format(cm[1,0]))
print('TP - True Positive {}'.format(cm[1,1]))
print('Accuracy Rate: {}' .format(np.divide(np.sum([cm[0,0],cm[1,1]]),np.sum(cm))*100))
print('Misclassification Rate: {}' .format(np.divide(np.sum([cm[0,1],cm[1,0]]),np.sum(cm))*100))


# In[266]:


sns.heatmap(confusion_matrix(y_test,sv_pred),annot=True,fmt="d")


# In[267]:


#classification report of Support Vector Machine
print('Classification Report of Support Vector Machine: \n', classification_report(y_test,sv_pred,digits=5))


# In[268]:


#Area under curve of Support Vector Machine
auc=roc_auc_score(y_test,sv_pred)
print("ROC AUC SCORE of Support Vector Machine is",auc)


# In[269]:


fpr,tpr,thresholds=roc_curve(y_test,sv_pred)
plt.plot(fpr,tpr,color='orange',label='ROC')
plt.plot([0,1],[0,1],color='darkblue',linestyle='--',label='ROC curve(area=%0.2f)'%auc)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Support Vector Machine')
plt.legend()
plt.grid()
plt.show()


# In[270]:


#confusion matrix of Decision tree
#MAKING CONFUSION MATRIX OF Decision tree
from sklearn.metrics import classification_report,confusion_matrix
from sklearn.metrics import accuracy_score,roc_auc_score,roc_curve
cm=confusion_matrix(y_test,dt_pred)

print('TN - True Negative {}'.format(cm[0,0]))
print('FP - False Positive {}'.format(cm[0,1]))
print('FN - False Negative {}'.format(cm[1,0]))
print('TP - True Positive {}'.format(cm[1,1]))
print('Accuracy Rate: {}' .format(np.divide(np.sum([cm[0,0],cm[1,1]]),np.sum(cm))*100))
print('Misclassification Rate: {}' .format(np.divide(np.sum([cm[0,1],cm[1,0]]),np.sum(cm))*100))


# In[271]:


sns.heatmap(confusion_matrix(y_test,dt_pred),annot=True,fmt="d")


# In[272]:


#classification report of Decision tree
print('Classification Report of Decision tree: \n', classification_report(y_test,dt_pred,digits=5))


# In[273]:


#Area under curve of Decision tree
auc=roc_auc_score(y_test,dt_pred)
print("ROC AUC SCORE of Decision tree is",auc)


# In[274]:


fpr,tpr,thresholds=roc_curve(y_test,dt_pred)
plt.plot(fpr,tpr,color='orange',label='ROC')
plt.plot([0,1],[0,1],color='darkblue',linestyle='--',label='ROC curve(area=%0.2f)'%auc)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Decision tree')
plt.legend()
plt.grid()
plt.show()


# In[275]:


#confusion matrix of Random forest
#MAKING CONFUSION MATRIX OF Random forest
from sklearn.metrics import classification_report,confusion_matrix
from sklearn.metrics import accuracy_score,roc_auc_score,roc_curve
cm=confusion_matrix(y_test,rf_pred)

print('TN - True Negative {}'.format(cm[0,0]))
print('FP - False Positive {}'.format(cm[0,1]))
print('FN - False Negative {}'.format(cm[1,0]))
print('TP - True Positive {}'.format(cm[1,1]))
print('Accuracy Rate: {}' .format(np.divide(np.sum([cm[0,0],cm[1,1]]),np.sum(cm))*100))
print('Misclassification Rate: {}' .format(np.divide(np.sum([cm[0,1],cm[1,0]]),np.sum(cm))*100))


# In[276]:


sns.heatmap(confusion_matrix(y_test,rf_pred),annot=True,fmt="d")


# In[277]:


#classification report of Random forest
print('Classification Report of Random forest: \n', classification_report(y_test,rf_pred,digits=5))


# In[278]:


#Area under curve of Random forest
auc=roc_auc_score(y_test,rf_pred)
print("ROC AUC SCORE of Random forest is",auc)


# In[279]:


fpr,tpr,thresholds=roc_curve(y_test,rf_pred)
plt.plot(fpr,tpr,color='orange',label='ROC')
plt.plot([0,1],[0,1],color='darkblue',linestyle='--',label='ROC curve(area=%0.2f)'%auc)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Random forest')
plt.legend()
plt.grid()
plt.show()


# In[ ]:


from tkinter import *
import joblib
import numpy as np
from sklearn import *
def show_entry_fields():
    p1=float(e1.get())
    p2=float(e2.get())
    p3=float(e3.get())
    p4=float(e4.get())
    p5=float(e5.get())
    p6=float(e6.get())
    p7=float(e7.get())
    p8=float(e8.get())
   
    model = joblib.load('model_joblib_diabetes')
    result=model.predict([[p1,p2,p3,p4,p5,p6,p7,p8]])
    
    if result == 0:
        Label(master, text="Non-Diabetic").grid(row=31)
    else:
        Label(master, text="Diabetic").grid(row=31)
    
    
master = Tk()
master.title("Diabetes Prediction Using Machine Learning")


label = Label(master, text = "Diabetes Prediction Using Machine Learning"
                          , bg = "black", fg = "white"). \
                               grid(row=0,columnspan=2)
Label(master, text="Pregnancies").grid(row=1)
Label(master, text="Glucose").grid(row=2)
Label(master, text="Enter Value of BloodPressure").grid(row=3)
Label(master, text="Enter Value of SkinThickness").grid(row=4)
Label(master, text="Enter Value of Insulin").grid(row=5)
Label(master, text="Enter Value of BMI").grid(row=6)
Label(master, text="Enter Value of DiabetesPedigreeFunction").grid(row=7)
Label(master, text="Enter Value of Age").grid(row=8)


e1 = Entry(master)
e2 = Entry(master)
e3 = Entry(master)
e4 = Entry(master)
e5 = Entry(master)
e6 = Entry(master)
e7 = Entry(master)
e8 = Entry(master)
e1.grid(row=1, column=1)
e2.grid(row=2, column=1)
e3.grid(row=3, column=1)
e4.grid(row=4, column=1)
e5.grid(row=5, column=1)
e6.grid(row=6, column=1)
e7.grid(row=7, column=1)
e8.grid(row=8, column=1)


Button(master, text='Predict', command=show_entry_fields).grid()

mainloop()






